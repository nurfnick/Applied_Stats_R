---
title: "Bootstrapping and Cross Validation"
author: "Nicholas Jacob"
date: "11/15/2020"
output: html_document
---

```{r setup, include = FALSE}
library(ggplot2)
library(ggExtra)
library(tidyverse)
library(GGally)
library(boot)
library(caret)
data = read.csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSDpJqmVSks0f4vLzzcmcTfPJ8TSu4ziCNpTFy_fIY6LibZksRXzCfJYXj9qZd4NiofejxoYSkmLMwu/pub?output=csv')

```

# Bootstrapping

Next we will take the same data and explore the bootstrapping method.  Many of you were introduced to this in Intro to Stats.  I'll repeat my hypothesis test that the mean ages of men and women olympic athletes are different.  If we establish the mean age of men and women olympians as $\mu_M$ and $\mu_W$ respectively, our null and alternative hypothesis will be:

$$
\begin{align}
H_0:\ \mu_M=\mu_W\\
H_A:\ \mu_M\neq\mu_W.
\end{align}
$$
I have added the library **boot** to my packages that I load at the beginning of the document and I'll **set.seed**, there is nothing special about the number you pick, if you change it you'll get another random, but repeatable action.  Any Douglas Adams fans know why I use 42?  In order to run the boot command you have to pass it a function that can be sub-divided.  Essentially you just have to pass it a function that slices the dataset.  Mimic what I do below and you should be golden!


```{r bootFullMean}
set.seed(42)
samp_mean <- function(x, i) {
  mean(x[i])
}
ages <- na.omit(data$Age) #needed to get rid of the NaN's
results <- boot(ages, samp_mean, 100)
plot(results)
```
```{r}
boot.ci(results, type="perc")
```
Let's repeat this but first divide the data based on Sex.
```{r}
menages <- na.omit(data[which(data$Sex == 'M'),'Age'])
womenages <- na.omit(data[which(data$Sex == 'F'),'Age'])
bootM <- boot(menages, samp_mean,100)
bootW <- boot(womenages, samp_mean, 100)
boot.ci(bootM, type = 'perc')
```
```{r}
boot.ci(bootW, type = 'perc')
```

We notice that the center of both confidence intervals fall outside of the other so we will reject the null hypothesis.  The mean age of men and women are different.

Let's look at a non-parametric statistic.  I am going to ask if the median age of winter athletes is different from summer athletes.

```{r}
winterAge <- na.omit(data[which(data$Season== 'Winter'),'Age'])
summerAge <- na.omit(data[which(data$Season== 'Summer'),'Age'])
median(winterAge)
```
```{r}
median(summerAge)
```

Well they both have the same sample median so I doubt they will be different but let's test it anyway.
$$
H_0; m_S=m_W\\
H_A: m_S\neq m_W
$$
```{r}
samp_median <- function(x,i){
  median(x[i])
}

bootW = boot(winterAge,samp_median,100)
boot.ci(bootW, type = 'perc')
```

```{r}
bootS = boot(summerAge, samp_median,100)
boot.ci(bootS, type = 'perc')
```

Since the Confidence intervals are the same, we will fail to reject the null hypothesis.  Thus we do not have evidence to suggest that the median age of winter and summer athletes is different.  It might have been best to do the median age of men and women but sometimes I just like to explore.  I should also note that in science a fail to reject is almost never reported.  This is sometimes called the 'file drawer problem' in that research that yields a 'fail to reject the null hypothesis' is never reported.  This is an issue because if an erroneous experiment is repeated 20 times, 19 of these will fail to reject but the one that does reject the null hypothesis will be published.  Sometimes this is also referred to as the reproducibility crisis.  It is a big problem in the social science literature!

# Cross Validation

I'd like to do cross validation in two ways.  I am first going to do the traditional split, 66%.  I'll fit the model to the training data and then check out the results on the remaining 34% testing data.  

```{r}
sports = c('Swimming', 'Tennis', 'Rowing', 'Gymnastics', 'Golf', 'Athletics', 'Bobsleigh')

dataLessSports <- data[which(data$Sport %in% sports),]

trainingSamples <- createDataPartition(dataLessSports$ID,p=.66,list = FALSE)
trainData <- dataLessSports[trainingSamples,]
testData <- dataLessSports[-trainingSamples,]

model <- lm(Age ~ Sport + Height + Weight, data = trainData, na.action = na.omit)
summary(model)
```

I specifically picked a silly model where I first restricted to a handful of sports.  Does anybody think we can predict the age of an athlete by their sport, height or weight?  You should notice though all of the coefficients are statistically significant.  Let's check out how this does on the testing data.

```{r}
predictions = predict(model,testData)

tes <- testData$Age[(!is.na(predictions)) & (!is.na(testData$Age))]
pre <- predictions[(!is.na(predictions)) & (!is.na(testData$Age))]

data.frame(R2 = R2(pre, tes ),
           RMSE = RMSE(pre, tes ))
```

I had to do some fancy work there to make this work because of all the na values.  Notice that I sliced both the prediction and the actual data cutting out the **NA** before I proceeded.  I've left my **NA** in my data to show you that while most stuff will work, you sometimes have to do heroics to get around it!  I'll also point out that this model does not do that poorly on testing data so maybe there is some reason to expect the sport to correlate to the age of the athlete although I will note that there is not really that big of swing except maybe between swimmers and golfers.  Athletics is the intercept so it does not have a coefficient.  

Let's repeat this but do the $k$-fold cross validation.  I'll use $k=10$ and run the exact same model ten times.  This time we'll train on 90% of the data and test on a slice of 10%.  Then we will reintroduce that 10% and hold back a different 10% of the data.  We repeat this ten times until all slices have been held back as the training data.
```{r}
dataNARemoved <- na.omit(dataLessSports)
```
This piece of code removed all the rows that had an NA values.  Now I think I am golden to run the cross validation
```{r}
train.control <- trainControl(method = "cv", number = 10)

model <- train(Age ~ Sport + Height + Weight, data = dataNARemoved, 
               method = "lm",
               trControl = train.control)
# Summarize the results
print(model)
```

Okay I have done it!  I doubt I am going to make a very acuarate prediction using this model but I will not that it is clear this model is not overfit.  I am confident that the model is not overfit because the cross validation gave very similar metrics to the original model.  There are no hard and fast rules here but if you see large swings, you have some evidence that the model is overfit (too dependant on a few values for the fit).