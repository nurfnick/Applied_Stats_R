---
title: "Cross Validation Project"
author: "Nicholas Jacob"
date: "11/30/2020"
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(caret)
data = read.csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSDpJqmVSks0f4vLzzcmcTfPJ8TSu4ziCNpTFy_fIY6LibZksRXzCfJYXj9qZd4NiofejxoYSkmLMwu/pub?output=csv')
```

# Cross Validation

I'd like to do cross validation in two ways.  I am first going to do the traditional split, 66%.  I'll fit the model to the training data and then check out the results on the remaining 34% testing data.  

```{r}
sports = c('Swimming', 'Tennis', 'Rowing', 'Gymnastics', 'Golf', 'Athletics', 'Bobsleigh')

dataLessSports <- data[which(data$Sport %in% sports),]

trainingSamples <- createDataPartition(dataLessSports$ID,p=.66,list = FALSE)
trainData <- dataLessSports[trainingSamples,]
testData <- dataLessSports[-trainingSamples,]

model <- lm(Age ~ Sport + Height + Weight, data = trainData, na.action = na.omit)
summary(model)
```

I specifically picked a silly model where I first restricted to a handful of sports.  Does anybody think we can predict the age of an athlete by their sport, height or weight?  You should notice though all of the coefficients are statistically significant.  Let's check out how this does on the testing data.

```{r}
predictions = predict(model,testData)

tes <- testData$Age[(!is.na(predictions)) & (!is.na(testData$Age))]
pre <- predictions[(!is.na(predictions)) & (!is.na(testData$Age))]

data.frame(R2 = R2(pre, tes ),
           RMSE = RMSE(pre, tes ))
```

I had to do some fancy work there to make this work because of all the na values.  Notice that I sliced both the prediction and the actual data cutting out the **NA** before I proceeded.  I've left my **NA** in my data to show you that while most stuff will work, you sometimes have to do heroics to get around it!  I'll also point out that this model does not do that poorly on testing data so maybe there is some reason to expect the sport to correlate to the age of the athlete although I will note that there is not really that big of swing except maybe between swimmers and golfers.  Athletics is the intercept so it does not have a coefficient.  

Let's repeat this but do the $k$-fold cross validation.  I'll use $k=10$ and run the exact same model ten times.  This time we'll train on 90% of the data and test on a slice of 10%.  Then we will reintroduce that 10% and hold back a different 10% of the data.  We repeat this ten times until all slices have been held back as the training data.
```{r}
dataNARemoved <- na.omit(dataLessSports)
```
This piece of code removed all the rows that had an NA values.  Now I think I am golden to run the cross validation
```{r}
train.control <- trainControl(method = "cv", number = 10)

model <- train(Age ~ Sport + Height + Weight, data = dataNARemoved, 
               method = "lm",
               trControl = train.control)
# Summarize the results
print(model)
```

```{r}
summary(model)
```


Okay I have done it!  I doubt I am going to make a very accuarate prediction using this model but I will not that it is clear this model is not overfit.  I am confident that the model is not overfit because the cross validation gave very similar metrics to the original model.  There are no hard and fast rules here but if you see large swings, you have some evidence that the model is overfit (too dependent on a few values for the fit).

Let's examine the plots.
```{r}
reg <- lm(Age~Height, data = dataNARemoved)
plot(dataNARemoved$Height,dataNARemoved$Age)
abline(reg,col = 'Red')
abline(24,.0575) 
```

I struggled to figure out how to visualize this.  Maybe you will have some better ideas.  Clearly the one factor and the multi-factor regressions should not be compared like this.  I also struggled to get the data directly out of the 10-fold cross validation so I copied the estimates by hand.  This is what I get for creating too complicated of a model!

# Keep it Simple Stupid

While there is nothing wrong with what I did above, I have made everything super complicated.  Rather that delete the above, I thought I'd just add another section and repeat the exercise on a simpler model.

I am going to look at **Height** as a predictor of **Weight**  We have seen this earlier as an excellent model.
```{r}
HWfit <- lm(Weight ~ Height, data = data, na.action = na.omit)

with(data,plot(Height, Weight))
abline(HWfit)

#ggplot(data = data, aes(Height,Weight)) +
#  geom_smooth(method = 'lm') +
#  geom_point()
```

```{r}
confint.lm(HWfit)
```
Okay so now I'll repeat the withholding of a third of the data and do the fitting and make predictions on the testing data.
```{r}

dataHW <- data[c('ID','Height','Weight')]
dataHW <-na.omit(dataHW)



trainingSamples <- createDataPartition(dataHW$ID,p=.66,list = FALSE)
trainData <- dataHW[trainingSamples,]
testData <- dataHW[-trainingSamples,]

model <- lm(Weight ~ Height, data = trainData)
summary(model)
```
I am going to add to the data frame a variable that ask whether the data was in the testing or the training data.  I'll use that to graph the linear regression

```{r}
dataHW[trainingSamples,'Test_Train']='training'
dataHW[-trainingSamples,'Test_Train']='testing'

ggplot(data = dataHW, mapping = aes(Height,Weight,color = Test_Train))+
  geom_jitter() +
  geom_smooth(method = lm)
```
Here I'll add the confidence intervals from the model.  

```{r}
confint.lm(model)
```
These confidence intervals are also super close.  Everything suggests that my model is not overfit.

Cool, now I'll make some predictions on the testing data
```{r}
pre = predict(model,testData)

tes <- testData$Weight

data.frame(R2 = R2(pre, tes ),
           RMSE = RMSE(pre, tes ))
```

Not a giant jump!  Let's check out the prediction on a famous Olympian, Micheal Phelps
```{r}
data[which(data$Name == 'Michael Fred Phelps, II'),]
mpPrediction <- predict(model,data[which(data$Name == 'Michael Fred Phelps, II'),])

```

We see that we under predict his weight.
```{r}
mpPrediction[1]-91

```

Okay I am going to leave it there.  The linear model clearly works.  Again when we run cross validation, we are really asking to check if the model is overfit.  We should not see that in a robust linear model but it may be possible if you have a few outliers that really mess things up!  Recall we are supposed to look for outliers before we fit a regression by looking at the QQ-Plots.
